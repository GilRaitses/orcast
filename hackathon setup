Note
Gil Raitses
Today at 10:10 am
13 min

Copy Summary

Summary

Transcript

Edit transcript
Keywords
Speakers
Speaker 1 (100%)
Deploy the agent to the cover. So it's kind of roughly high level, three steps like that. Usually, I mean, you can do whatever you want, but as long as you meet the core requirements. So let me actually hand this over to Amit, and he's going to go through some of the same code.
Great. Yeah. Thanks so much, Lisa, thank you, James. So welcome everyone. My name is Amit, yeah. So before we get started, I have some important question to ask you. Like, has anyone been keeping up with the Coldplay drama? Yeah?
Okay, cool. So I know it's like been super controversial recently, but what I've been enjoying the most about it is that people are on the internet like, I don't know, like, anytime I scroll through Tiktok, I'll see like, a funny game or whatever. But what is most kind of like, funny about anything that I see online is the comments. And I'm pretty sure everyone can, like, agree. So the internet, like, ends up being so funny around this. And one of the things that I saw that someone created was a game they like by coded a game. You saw that one, right? Yeah.
So anyways, if you're interested, you can go check it out. Go like, type in Coldplay by coded game. And essentially you can move your cursor around to look around an audience, like an AI generated audience, to see which people are, like, having an affair. So anyways, like, it's, yeah, whatever. So the reason I mentioned this is because obviously agentic AI is kind of at the crux of everything that we seem to be building nowadays. And previously, you know, whenever some controversy or drama would happen online, someone would make a funny meme about it. But now people are making games about people making applications and all sorts of random AI generated stuff. So anyways, today I won't show you how to make a game like that, but I will show you how to set up your own kind of AI agentic application that's deployed on Cloud. Run GPU. So before I get started, does everyone know what a GPU is? I figured maybe we should ask this before actually get in.
Okay, so like just to explain it a little bit better, a GPU is a hardware device that allows you to run your inference for your LLM faster. It actually allows you to train your llms faster, but for the most part, we'll just be doing inference. GPUs have an innate ability, because they can process matrix operations faster than a regular CPU to do inference faster. What this means is that you can query
your LLM that's hosted on a GPU, like all open AI's. Llms, for example, are all hosted on an array of GPUs, and allows you to get your result back a lot faster. So that's why we're very interested in deploying these AI models to anything that has GPU support. So as of recently, cloud run introduced the GPU option. It allows you to deploy to cloud run using GPU and take advantage of that acceleration. So let's take a look at how we do it. Today, we're going to I'll give you a code sample that allows you to build and deploy your own kind of hackathon agent. But of course, you can make any agent that you want. The example that I'm giving you here today is just an agent that answers questions about today's hackathon. Very simple, probably won't be helpful overall, but gives you a good idea of how you can build your own agent. But there are three things that we're going to be building within the agent. So the agent should be able to communicate with other cloud run services, which is important if you're building like a service oriented, architecture based application or a multi agent based application. The second requirement is that it utilizes cloud run GPUs, and then we're going to build it on an agent framework. The agent framework that we're going to use today is ADK agent development kit, which is introduced by Google. However, you're welcome to use any agent framework that you want. You can use Lang chain, Lang graph. You can use QAI, whatever. I'll show you an example with APK.
So yeah, here's what the architecture looks like. Very, very simple.
We're gonna have our hackathon agent. That little icon there that's above the names, is collect the cloud run logo, and then Gemma, which will be posted on Cloud run as well. So first we're gonna have a request. So we're gonna say, hey hackathon agent, I want help with something. And then the hackathon agent will say, Hey, Gemma, I want to get help with this thing, because this person asked me. And then Gemma will respond. Gemma obviously being the LLM that has the ability to understand language, and then the hackathon agent will respond back. So seems like a bit of a redundant request. However, this is very this is a very important architecture in the world of multi agent systems, because you can then have this hackathon agent, which can essentially be an orchestrator, if you will communicate with other agents in the future, if you build more. In our case, here, we have Gemma Hosted on Cloud run that can be used as an LLM, which is one of the requirements that we have for today. So let's take a look at how we can deploy this, because it is quite simple. The first thing that you want to do is make sure you have the Google Cloud CLI installed, so if you don't have it already, you can run that first the first little line there the curl the SDK, and then run it. And then you want to authenticate within your terminal or your local terminal. And all this code is available in the sample repo, so feel free to take a look at that. And the sample repo is also available through the handbook. The link to it, and then you want to authenticate. So this is to make sure that your terminal has access to your GCP project. And remember, if you were setting up a Google Cloud project earlier, before we got started, you would have noticed that it asked you to create a project, or you would have had to create a project. Your project ID is where you would put it here. That is helpful in knowing because you can have many different projects within GCP, which are kind of like workspaces. And so when you deploy, you just want to make sure whatever project you're deploying to suits the project ID that you're interested in, right? And then, yeah, sorry, I have a little mistake here, but we're going to be using Europe West four, so you'll have G Cloud Config, and then you set your Google Cloud run region to be Europe West four. And so that way anything that we deploy on Google Cloud run will be deployed to Europe West four, OK. And then we enable some APIs, which is the last line here. So any questions with this before we kind of move forward.
So all of this should run successfully. The only time you'll have to kind of intervene is when you type in the Google Cloud auth login, it'll pop open a little browser for you and ask you to log in with your Gmail account, the one that you used to set up the project with okay. So this is one example of how you can build your LLM using Gemma or GEMMA on Cloud, run GPU. However, there are other options, like was mentioned by James earlier. This one uses a Docker file and Olam. Mom. Does anyone know what llama is?
Okay? So for those of us who don't, that's absolutely okay, olama is serving an LLM serving framework that we can use to run llms locally, but also with the ability with Docker, we can deploy o llama to serve the LLM for us on the cloud. So this Docker file allows us to create an olama It's installation that serves Gemma. So to pull Gemma, we're using Gemma three, 1b, here, just a small one, so that you can get it working. But you can, you know, change this if you want to to Gemma three, 4b, 4 billion parameters, and then run it. So very simple. And then the the key here is the last command, which is the G Cloud run deploy. So that first little parameter there, olama Gemma, is just the name of the service that we want to deploy. So we're just going to call it olama Gemma. And then there's a bunch of different flags here that we're going to run alongside it too. The one that's very important that you kind of have to keep in mind is the GPU one. And then, sorry, GPU type as well. So these two will indicate that we want to deploy
something to cloud run, but we want cloud run to give us a GPU to run.
Any questions here?
Okay, so this, this method? Yeah, sorry.
No, I think like you can leave it the way it is here. You can even remove concurrency for if you want to.
Anyways, all the code is in the sample repo. You can follow it, and depends on how much you're interacting with the cloud run instance. But for the most part, the biggest lift will be on the GPU, so you don't really have to worry too much about the CPU here. The other thing that I wanted to note as well is the no allow authenticated. So if you have ever worked with Google products before, especially deploying to cloud run, you may have run into this flag before. No allow authenticated just says that you cannot interact with this service unless you're authenticated. Okay, so pretty straightforward. You don't want anonymous folks to be able to interact with it. In our hackathon today and tomorrow, I would suggest that you actually change this flag to remove the nose. So just do it allow unauthenticated The reason being is because we want to be we don't want to be limited by the amount of time that we have messing around with authentication. It may development a little bit more challenging. Obviously, you want the security if you're putting something into production. And so if you do take your your architecture after the hackathon, you want to make it, put it into production, I highly suggest you do run the Gil, authenticate in and you authenticate your services. And I'll show you like. The code sample includes how to call the the cloud run services with authentication. However, it's a lot easier if you just do it without like, if you remove this note here, so it'll just be hyphen, hyphen. Allow unauthenticated, okay? So that way the service can be interacted with locally and when you deploy to the cloud.
Okay, so this is the Obama way, but we also have pre built containers for you. This technique makes it so that you make your own custom container that pulls in the olama instance, or, sorry, the LLM version that you want. So Gemma three, 1b in this case, however, we do have pre built containers, which are a little faster and a little easier. If you go on the handbook and the GPU config example, you'll see the first option there is the simpler, faster way. That was the one that James was mentioning, and it's all documented, super easy. When you're finished, you can actually test it by running these commands here. If you deploy using allow unauthenticated you actually don't have to run the proxy, if a proxy just makes it so that you proxy from your local computer into GCP, within the VPC, the Virtual Private kind of network, so that you can access the cloud, run service without having to run the whole authentication
run into all the authentication challenges. But you don't need to run the proxy if you're just using Cloud authenticated. So if you're not running the proxy and you still want to do a test using curl, you just sub in the URL that was deployed to. So when cloud run deploys, when it finishes running the command, it'll give you a little URL there, and in the code actually show you a little way to pull that URL. If you don't, there's like, three different ways to get the URL. One is after it deploys, it shows you. Two is if you go on the platform, the console, type in cloud, run, you'll see it there. And three, there's a little command that I give you in the code to get the URL, and then you just up in the URL there, and then run this, and it'll give you the response. So the worst, and you know that your your service is deployed successfully.
Okay, any questions here?
Okay, cool. So finally, this is one option that you can use to integrate the deployed service into your code. However, there are many different ways for you to interact with your service. It is essentially whatever you have deployed a URL so you can, if you've been familiar with software engineering in the past with web development, if you've ever interacted with an API, you can interact with the service in the same exact way that you would interact with any other API. The only caveat is, if you've deployed it with security no allow authenticated, unauthenticated. Sorry, you have to make sure you get the ID token for your for your account, so that you can attach it to the header, so that you can make an authenticated request. But if you're using the no allow authenticated, I'm sorry, the allow unauthenticated, then it's fine. You don't have to do the authentication thing. And then this is using one of our client, SDKs, in order to run that request to to the service. However, if you are familiar with like you're writing in Python, and you're familiar with the request framework or the request package, you can use that to to hit the API all the same way. So yeah, here's another example,
and that's it. So now we can get to the fun part, but before that, I think we have one final note. And then, yeah, so does anyone have any questions for me before I step aside?
So the sample repo is in the handbook. The Handbook link is in the portal for the AI tinkerers portal. So if you just go into your dashboard, you'll see the link to the handbook there, which includes the link
through both.
