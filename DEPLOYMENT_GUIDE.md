# ORCAST ML Service - Deployment Guide

## **üéâ Production-Ready Behavioral Prediction API**

Your ORCAST ML service is now complete and ready for deployment! This guide covers local testing, Cloud Run deployment, and API usage.

## **üìä Service Overview**

### **Capabilities**
- **Real-time Behavioral Classification**: 13D environmental features ‚Üí orca behavior prediction
- **Feeding Success Prediction**: When feeding behavior is predicted, estimates success probability
- **Batch Processing**: Multiple predictions in a single request
- **High Performance**: Average response time ~13ms
- **Production Ready**: FastAPI with proper error handling, validation, and monitoring

### **Behavioral Classes**
- `feeding` - Active foraging/hunting behavior
- `socializing` - Pod interaction and play
- `traveling` - Directional movement
- `resting` - Low activity/stationary
- `unknown` - Unclear or mixed behaviors

## **üöÄ Quick Start**

### **1. Local Testing (Already Running)**
Your service is currently running at `http://localhost:8080`

```bash
# Health check
curl http://localhost:8080/health

# Get example data
curl http://localhost:8080/example

# Make a prediction
curl -X POST http://localhost:8080/predict \
  -H "Content-Type: application/json" \
  -d '{
    "latitude": 48.5,
    "longitude": -123.0,
    "pod_size": 3,
    "water_depth": 50.0,
    "tidal_flow": 0.2,
    "temperature": 15.5,
    "salinity": 30.1,
    "visibility": 20.0,
    "current_speed": 0.5,
    "noise_level": 120.0,
    "prey_density": 0.6,
    "hour_of_day": 14,
    "day_of_year": 200
  }'
```

### **2. Test Performance**
```bash
# Run comprehensive test suite
source venv/bin/activate
python3 test_ml_service.py
```

## **‚òÅÔ∏è Cloud Run Deployment**

### **Option A: Deploy with gcloud CLI**

```bash
# 1. Create Dockerfile (already provided below)
# 2. Build and deploy
gcloud run deploy orcast-ml-service \
  --source . \
  --region us-central1 \
  --allow-unauthenticated \
  --memory 2Gi \
  --cpu 2 \
  --max-instances 10 \
  --port 8080
```

### **Option B: Deploy via Docker**

```bash
# Build container
docker build -t orcast-ml-service .

# Test locally
docker run -p 8080:8080 orcast-ml-service

# Push to Google Container Registry
docker tag orcast-ml-service gcr.io/orca-466204/orcast-ml-service
docker push gcr.io/orca-466204/orcast-ml-service

# Deploy to Cloud Run
gcloud run deploy orcast-ml-service \
  --image gcr.io/orca-466204/orcast-ml-service \
  --region us-central1 \
  --allow-unauthenticated
```

## **üê≥ Dockerfile**

Create this `Dockerfile` in your project root:

```dockerfile
FROM python:3.11-slim

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code and models
COPY deploy_behavioral_ml_service.py .
COPY models/ ./models/

# Set environment variables
ENV PORT=8080
ENV PYTHONPATH=/app

# Expose port
EXPOSE 8080

# Run the application
CMD ["python", "deploy_behavioral_ml_service.py"]
```

## **üìù requirements.txt for Deployment**

Add these dependencies to your `requirements.txt`:

```
fastapi==0.116.1
uvicorn==0.35.0
pydantic==2.11.7
numpy==2.3.1
pandas==2.2.3
scikit-learn==1.7.1
joblib==1.5.1
```

## **üîß API Endpoints**

### **Core Endpoints**

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Service info and health |
| `/health` | GET | Detailed health check |
| `/predict` | POST | Single behavior prediction |
| `/predict/batch` | POST | Batch predictions |
| `/predict/simple` | POST | Simplified prediction |
| `/example` | GET | Example input data |
| `/models/info` | GET | Model information |

### **Prediction Input Format**

```json
{
  "latitude": 48.5,
  "longitude": -123.0,
  "pod_size": 3,
  "water_depth": 50.0,
  "tidal_flow": 0.2,
  "temperature": 15.5,
  "salinity": 30.1,
  "visibility": 20.0,
  "current_speed": 0.5,
  "noise_level": 120.0,
  "prey_density": 0.6,
  "hour_of_day": 14,
  "day_of_year": 200
}
```

### **Prediction Output Format**

```json
{
  "predicted_behavior": "unknown",
  "confidence": 0.587,
  "probabilities": {
    "feeding": 0.156,
    "resting": 0.000,
    "socializing": 0.237,
    "traveling": 0.020,
    "unknown": 0.587
  },
  "feeding_success_probability": null,
  "model_version": "2025-07-19T17:12:12.894189",
  "prediction_timestamp": "2025-07-19T17:18:17.019473"
}
```

## **üéØ Usage Examples**

### **Python Client**

```python
import requests

# Make prediction
response = requests.post(
    "http://localhost:8080/predict",
    json={
        "latitude": 48.5,
        "longitude": -123.0,
        "pod_size": 3,
        "water_depth": 50.0,
        "tidal_flow": 0.2,
        "temperature": 15.5,
        "salinity": 30.1,
        "visibility": 20.0,
        "current_speed": 0.5,
        "noise_level": 120.0,
        "prey_density": 0.6,
        "hour_of_day": 14,
        "day_of_year": 200
    }
)

prediction = response.json()
print(f"Predicted behavior: {prediction['predicted_behavior']}")
print(f"Confidence: {prediction['confidence']:.3f}")
```

### **JavaScript Client**

```javascript
const response = await fetch('http://localhost:8080/predict', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    latitude: 48.5,
    longitude: -123.0,
    pod_size: 3,
    water_depth: 50.0,
    tidal_flow: 0.2,
    temperature: 15.5,
    salinity: 30.1,
    visibility: 20.0,
    current_speed: 0.5,
    noise_level: 120.0,
    prey_density: 0.6,
    hour_of_day: 14,
    day_of_year: 200
  })
});

const prediction = await response.json();
console.log(`Predicted behavior: ${prediction.predicted_behavior}`);
```

## **üìà Performance Metrics**

Based on test results:
- **Average Response Time**: ~13ms
- **Min Response Time**: ~11ms
- **Max Response Time**: ~23ms
- **Batch Processing**: 3 predictions in ~28ms
- **Success Rate**: 100% in testing

## **üîç Model Information**

### **Training Data**
- **Records**: 327 whale sightings from BigQuery
- **Features**: 13-dimensional environmental vectors
- **Time Range**: Recent whale activity data
- **Quality**: Expert-validated behavioral classifications

### **Model Performance**
- **Primary Classifier**: Random Forest (73.1% CV accuracy)
- **Success Predictor**: Gradient Boosting (57.1% accuracy)
- **Classes**: 5 behavioral categories
- **Preprocessing**: StandardScaler normalization

### **Feature Importance**
1. `prey_density` - Most predictive of feeding behavior
2. `noise_level` - Affects all behavioral decisions
3. `hour_of_day` - Temporal patterns in behavior
4. `pod_size` - Social behavior indicator
5. `temperature` - Environmental preference

## **üö¶ Monitoring & Health**

### **Health Check Response**
```json
{
  "status": "healthy",
  "models": {
    "behavior_model": true,
    "success_model": true,
    "scaler": true,
    "encoder": true
  },
  "classes": ["feeding", "resting", "socializing", "traveling", "unknown"],
  "metadata": {...}
}
```

### **Production Monitoring**
- Monitor `/health` endpoint for model status
- Track prediction latency and error rates
- Monitor memory usage (models ~1.2MB total)
- Set up alerts for failed predictions

## **üîê Security Considerations**

- **Input Validation**: Pydantic models enforce data types and ranges
- **Rate Limiting**: Consider adding rate limiting for production
- **Authentication**: Add API keys or OAuth for production use
- **CORS**: Currently allows all origins (adjust for production)

## **üìö Next Steps**

1. **Deploy to Cloud Run**: Use the provided Docker configuration
2. **Set up Monitoring**: Configure Cloud Monitoring and logging
3. **Add Authentication**: Implement API key management
4. **Scale Testing**: Test with higher prediction volumes
5. **Model Updates**: Implement model versioning and rolling updates

## **üéâ Ready for Production!**

Your ORCAST ML service is fully functional and ready for real-world orca behavioral predictions. The API provides reliable, fast predictions based on environmental conditions with confidence scores and detailed probability distributions.

**Current Status**: ‚úÖ Running locally at `http://localhost:8080`  
**Next Action**: Deploy to Cloud Run for production use 